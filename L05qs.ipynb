{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L05qs.ipynb",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhyeIVDosAzy"
      },
      "source": [
        "In this lesson, we trained a linear neural network to distinguish background green pixels from other foreground pixels. Do you think the network will converge to the same weights every time it is run? Try it and see. Is there any property of the weights that seems consistent from one run to another?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PFoSOH4XZiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39646f31-fec0-4950-a95c-be988fb03f73"
      },
      "source": [
        "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/imgs/greenML.png"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-09 22:08:17--  https://github.com/mlittmancs/great_courses_ml/raw/master/imgs/greenML.png\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mlittmancs/great_courses_ml/master/imgs/greenML.png [following]\n",
            "--2023-09-09 22:08:17--  https://raw.githubusercontent.com/mlittmancs/great_courses_ml/master/imgs/greenML.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1190198 (1.1M) [image/png]\n",
            "Saving to: ‘greenML.png’\n",
            "\n",
            "greenML.png         100%[===================>]   1.13M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-09-09 22:08:17 (20.7 MB/s) - ‘greenML.png’ saved [1190198/1190198]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmxJz39WsEDO"
      },
      "source": [
        "Download the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLDbTVcmDZFl"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "img = keras.utils.load_img(\"greenML.png\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx5w3UgAE2cS"
      },
      "source": [
        "Trim the edges of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5ny7M6UflUb"
      },
      "source": [
        "arr = image.img_to_array(img)\n",
        "# Trim off edges\n",
        "arr = arr[:697,:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ShPb7xE7Rj"
      },
      "source": [
        "Isolate out the bacground and convert it to a dataset of positive examples, `yesList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrdg39l6GMtH"
      },
      "source": [
        "# background\n",
        "tmp = arr[:,:360]\n",
        "\n",
        "yesList = np.reshape(tmp,(-1,3))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eHUJXK6HVwh"
      },
      "source": [
        "Print the number of values in each dimension of `tmp`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbLvsTBgKnC9"
      },
      "source": [
        "Isolate out the foreground and make a dataset of foreground pixels called `noList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcSqKYtDwqsq"
      },
      "source": [
        "# foreground\n",
        "tmp = arr[30:,547:620]\n",
        "\n",
        "noList = np.reshape(tmp,(-1,3))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mluPsOFNL_i8"
      },
      "source": [
        "Finalize our dataset, with a variable `alldat`, which contains our list of pixels, and `labs`, which is our list of labels for each pixel---`0` for green background pixels and `1` for foreground pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfPVKGHoenU2"
      },
      "source": [
        "# Build a list of pixels for both positive and negative examples.\n",
        "alldat = np.concatenate((yesList,noList))\n",
        "\n",
        "# labels\n",
        "labs = np.concatenate((np.ones(len(yesList)), np.zeros(len(noList))))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlYPmhdkMDIQ"
      },
      "source": [
        "Build a classifier to separate the background from the foreground.\n",
        "\n",
        "We define a `loss` function.  The `loss` takes in our data `alldat`, our labels `labs`, and our current weights `w`.  Make adjustments to `w` based on this loss function.  To make a prediction, multiply `w` by `alldat`, and pass it through a sigmoid function to get our predictions `y`.  Then, compare our predictions `y` to their true labels `labs` using a squared loss, the sum of the squared difference between our true labels `labs`, and our predicted values `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z03-JVB_eqHJ"
      },
      "source": [
        "# Add an additional column to the data corresponding to\n",
        "#  the offset parameter.\n",
        "alldat = np.concatenate((alldat,np.ones((len(alldat),1))),1)\n",
        "\n",
        "# Compute the loss of the rule specified by weights w with respect\n",
        "#  to the data alldat labeled with labs\n",
        "def loss(w, alldat, labs):\n",
        "  # Compute a weighted sum for each instance\n",
        "  h = np.matmul(alldat,w)\n",
        "  # transform the sum using the sigmoid function\n",
        "  y = 1/(1 + np.exp(-h))\n",
        "  # take the difference between the labels and the output of the\n",
        "  #  sigmoid, squared, then sum up over all instances to get the\n",
        "  #  total loss.\n",
        "  loss = np.sum((labs - y)**2)\n",
        "  return(loss)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vkgAKUMNFC"
      },
      "source": [
        "Train the model, by creating a `fit` function to fit the model to the data. Update the weights through gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l44hJI6tXvox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d01701-5f94-4285-b6cb-a576f2f9dce6"
      },
      "source": [
        "def fit(w,alldat,labs):\n",
        "  # alpha represents how big of a step we’ll\n",
        "  #  be taking in the direction of the derivative.\n",
        "  #  It’s called the learning rate.\n",
        "  alpha = 0.1\n",
        "\n",
        "  # We'll stop searching when we're at a (near) local min\n",
        "  done = False\n",
        "  while not done:\n",
        "    # Every 100 iterations or so, let’s\n",
        "    #  take a peek at the weights, the learning\n",
        "    #  rate, and the current loss\n",
        "    if np.random.random() < 0.01: print(w, alpha, loss(w,alldat,labs))\n",
        "    # The next few lines compute the gradient\n",
        "    #  of the loss function. The details aren’t\n",
        "    #  important right now.\n",
        "    # delta_w is the change in the weights\n",
        "    #  suggested by the gradient\n",
        "    h = np.matmul(alldat,w)\n",
        "    y = 1/(1 + np.exp(-h))\n",
        "    delta_w = np.add.reduce(np.reshape((labs-y) * np.exp(-h)*y**2,(len(y),1)) * alldat)\n",
        "    # if we take a step of size alpha and update\n",
        "    #  the weights, we’ll get new weights neww.\n",
        "    current_loss = loss(w,alldat,labs)\n",
        "    alpha *= 2\n",
        "    neww = w + alpha* delta_w\n",
        "    while loss(neww,alldat,labs) >= current_loss and not done:\n",
        "      alpha /= 2\n",
        "      if alpha*max(abs(delta_w)) < 0.0001:\n",
        "        done = True\n",
        "        print(alpha,delta_w)\n",
        "      else: neww = w + alpha* delta_w\n",
        "    if not done: w = neww\n",
        "  return(w)\n",
        "\n",
        "w = np.random.random(4)\n",
        "w = fit(w,alldat,labs)\n",
        "print(w)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-43a48f8a219c>:11: RuntimeWarning: overflow encountered in exp\n",
            "  y = 1/(1 + np.exp(-h))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.18334597 -0.1838335   0.83186319  0.51567514] 2.44140625e-05 48564.363224595945\n",
            "[-0.87474947  0.07668323 -0.1783271   0.42942369] 3.0517578125e-06 507.77226527317634\n",
            "[-0.87752763  0.08407091 -0.20889486  0.42331113] 6.103515625e-06 495.8377839872295\n",
            "[-0.88328776  0.09717871 -0.24869324  0.41132478] 1.220703125e-05 486.10499937754594\n",
            "[-0.88737659  0.10183956 -0.26892571  0.40304967] 6.103515625e-06 482.578372616331\n",
            "[-0.89935188  0.11437686 -0.31021545  0.37933802] 1.220703125e-05 477.12836403526\n",
            "[-0.90447007  0.1177554  -0.324012    0.36933359] 1.220703125e-05 475.620460637414\n",
            "[-0.9053911   0.11859153 -0.32624807  0.3675404 ] 6.103515625e-06 475.35847223909764\n",
            "[-0.91315335  0.12346176 -0.34389076  0.35247579] 1.220703125e-05 473.5573270338417\n",
            "[-0.94480381  0.13852717 -0.39319062  0.29150701] 1.220703125e-05 468.589714072507\n",
            "[-0.97247681  0.14588299 -0.41656275  0.237679  ] 2.44140625e-05 465.66868804329164\n",
            "[-1.01240273  0.15022757 -0.43126166  0.15683332] 1.220703125e-05 462.0678474321168\n",
            "[-1.01693188  0.15106703 -0.43176638  0.14731532] 1.220703125e-05 461.68687541143777\n",
            "[-1.02306199  0.15079619 -0.43260358  0.13428805] 1.220703125e-05 461.14137268896894\n",
            "[-1.0317116   0.15132958 -0.43303652  0.11562093] 6.103515625e-06 460.38239160931425\n",
            "[-1.03827788  0.15119028 -0.43318942  0.1012039 ] 1.220703125e-05 459.82050745741543\n",
            "[-1.04746888  0.15111702 -0.43292836  0.08064309] 1.220703125e-05 459.031613350692\n",
            "[-1.05442045  0.1514603  -0.43230149  0.06477649] 6.103515625e-06 458.39711986306895\n",
            "[-1.0564604   0.15154377 -0.43207512  0.06006549] 2.44140625e-05 458.220878984743\n",
            "[-1.07618639  0.15126312 -0.42920773  0.01310789] 1.220703125e-05 456.4771833157373\n",
            "[-1.07716892  0.15118162 -0.42904067  0.0106979 ] 1.220703125e-05 456.38234122784127\n",
            "[-1.07806995  0.150572   -0.42904243  0.008479  ] 1.220703125e-05 456.29066017467306\n",
            "[-1.07904532  0.15057437 -0.42884419  0.00607329] 1.220703125e-05 456.19839242067184\n",
            "[-1.08803749  0.15002059 -0.42700958 -0.01645319] 1.220703125e-05 455.3889214344493\n",
            "[-1.09679043  0.15008491 -0.42474529 -0.03900901] 1.220703125e-05 454.5561749807338\n",
            "[-1.13107057  0.14708915 -0.41408363 -0.13434762] 1.220703125e-05 451.2201151708517\n",
            "[-1.13156944  0.14700708 -0.41390663 -0.1358283 ] 1.220703125e-05 451.1735625743661\n",
            "[-1.17512734  0.14251    -0.39302637 -0.27828819] 1.220703125e-05 446.4143661638287\n",
            "[-1.18080673  0.14126701 -0.38964774 -0.29913222] 1.220703125e-05 445.731280488618\n",
            "[-1.18583807  0.14044308 -0.38635912 -0.31812234] 1.220703125e-05 445.11745197789975\n",
            "[-1.18728159  0.14044977 -0.38530567 -0.32366513] 1.220703125e-05 444.93615554537473\n",
            "[-1.19262027  0.1396097  -0.38149083 -0.34455516] 1.220703125e-05 444.26867982216254\n",
            "[-1.20144391  0.13743166 -0.3747768  -0.38050816] 2.44140625e-05 443.12321848160144\n",
            "[-1.20874635  0.13602802 -0.36842999 -0.41172698] 1.220703125e-05 442.11600016677284\n",
            "[-1.20939318  0.13588358 -0.36784133 -0.41456106] 1.220703125e-05 442.02561531446025\n",
            "[-1.21027791  0.13590429 -0.36696534 -0.41845542] 1.220703125e-05 441.90250181584423\n",
            "[-1.21163541  0.13521776 -0.36580509 -0.42447653] 2.44140625e-05 441.7182725780917\n",
            "[-1.21427412  0.13463269 -0.36326616 -0.43632888] 1.220703125e-05 441.3375134937346\n",
            "[-1.21766899  0.1338899  -0.35985625 -0.45188185] 1.220703125e-05 440.83802418946277\n",
            "[-1.21941213  0.1336898  -0.3579916  -0.4600047 ] 1.220703125e-05 440.58280589892496\n",
            "[-1.21952522  0.1334118  -0.35794411 -0.4605361 ] 1.220703125e-05 440.56302303521824\n",
            "[-1.21990116  0.13321462 -0.35757939 -0.46230243] 1.220703125e-05 440.511370323249\n",
            "[-1.22386149  0.13220492 -0.35331463 -0.48118435] 1.220703125e-05 439.9089761767792\n",
            "[-1.22524141  0.13168176 -0.35182583 -0.48788687] 2.44140625e-05 439.7054182724679\n",
            "[-1.24045682  0.12716581 -0.33356909 -0.56638975] 1.220703125e-05 437.22029343157584\n",
            "[-1.2416103   0.12714256 -0.33201797 -0.57271073] 1.220703125e-05 437.0208344562003\n",
            "[-1.24224685  0.12659447 -0.33131146 -0.57622491] 1.220703125e-05 436.91217140037526\n",
            "[-1.24297467  0.12641179 -0.3303774  -0.58026226] 1.220703125e-05 436.78325313285035\n",
            "[-1.24451127  0.12586568 -0.3284499  -0.58886098] 1.220703125e-05 436.51778966398575\n",
            "[-1.24792249  0.12515832 -0.32404556 -0.60831539] 1.220703125e-05 435.9122343713139\n",
            "[-1.25097487  0.12393592 -0.32032819 -0.62616591] 1.220703125e-05 435.35675884239663\n",
            "[-1.25455422  0.12273654 -0.31602889 -0.64764195] 1.220703125e-05 434.70468056108206\n",
            "[-1.2620162   0.12069195 -0.30762025 -0.69436607] 1.220703125e-05 433.29356886190794\n",
            "[-1.26233783  0.12069475 -0.30725684 -0.69644052] 1.220703125e-05 433.2276618545136\n",
            "[-1.269162    0.11907622 -0.30058646 -0.7416749 ] 1.220703125e-05 431.89505753643465\n",
            "[-1.270685    0.11897737 -0.299163   -0.75209171] 1.220703125e-05 431.5921896115224\n",
            "[-1.27152458  0.11884168 -0.29842462 -0.75788542] 1.220703125e-05 431.425880865767\n",
            "[-1.27543633  0.1178077  -0.29532168 -0.78536005] 1.220703125e-05 430.636860534688\n",
            "[-1.28495158  0.11674552 -0.28847195 -0.85562019] 1.220703125e-05 428.66834534714695\n",
            "[-1.28812132  0.11578418 -0.28664944 -0.88015465] 1.220703125e-05 427.99965756174424\n",
            "[-1.28959042  0.11593346 -0.28567024 -0.89172259] 6.103515625e-06 427.66639021117055\n",
            "[-1.29163159  0.11580772 -0.28444034 -0.90801014] 1.220703125e-05 427.22803732159065\n",
            "[-1.29184231  0.1157527  -0.28432799 -0.90970605] 2.44140625e-05 427.18083475826484\n",
            "[-1.29192243  0.11580381 -0.28426424 -0.91035133] 1.220703125e-05 427.1664318829\n",
            "[-1.29223246  0.11538782 -0.28419619 -0.91285425] 1.220703125e-05 427.1026741093242\n",
            "[-1.29554528  0.11496347 -0.28235945 -0.93995436] 1.220703125e-05 426.38318290614603\n",
            "[-1.29750647  0.11489336 -0.28125967 -0.95632263] 1.220703125e-05 425.93802444666863\n",
            "[-1.29788226  0.11514311 -0.28097568 -0.95948625] 1.220703125e-05 425.8533881610174\n",
            "[-1.29839656  0.11492653 -0.28074994 -0.96383298] 1.220703125e-05 425.73561528380037\n",
            "[-1.29901894  0.11500909 -0.2803792  -0.96911505] 2.44140625e-05 425.5988460351365\n",
            "[-1.30076968  0.11446647 -0.27957598 -0.98411294] 1.220703125e-05 425.2178083269981\n",
            "[-1.3017799   0.11487452 -0.27891174 -0.99285795] 1.220703125e-05 424.9866008767785\n",
            "[-1.30345856  0.11451757 -0.27811945 -1.00754682] 6.103515625e-06 424.59624097662316\n",
            "[-1.30712436  0.11427796 -0.27628577 -1.04030593] 2.44140625e-05 423.75951690453405\n",
            "[-1.30726741  0.11412519 -0.2762575  -1.04160423] 1.220703125e-05 423.7250545528853\n",
            "[-1.30859303  0.11390629 -0.27564787 -1.05370258] 1.220703125e-05 423.4232735530953\n",
            "[-1.30916978  0.11421371 -0.27526893 -1.05900527] 1.220703125e-05 423.29239188743867\n",
            "[-1.31122932  0.11372406 -0.27438955 -1.07814936] 1.220703125e-05 422.80902720072595\n",
            "[-1.31293991  0.11383253 -0.27352414 -1.09429374] 2.44140625e-05 422.406556795262\n",
            "[-1.31572609  0.11356676 -0.27226811 -1.12108256] 6.103515625e-06 421.74669042010686\n",
            "[-1.31866289  0.11352673 -0.27090638 -1.15000053] 1.220703125e-05 421.0522766049607\n",
            "[-1.3197965   0.11313773 -0.27049745 -1.16135742] 1.220703125e-05 420.77719957468025\n",
            "[-1.31988959  0.11315129 -0.27045076 -1.1622947 ] 1.220703125e-05 420.75382521007685\n",
            "[-1.32096241  0.11322876 -0.26994032 -1.17315099] 6.103515625e-06 420.49255159840715\n",
            "[-1.32112444  0.11333652 -0.26983571 -1.17479894] 2.44140625e-05 420.45724634578977\n",
            "[-1.32317197  0.1129601  -0.26902527 -1.19582966] 1.220703125e-05 419.9589823034803\n",
            "[-1.3240359   0.11313989 -0.26858954 -1.20481463] 2.44140625e-05 419.74846317574287\n",
            "[-1.32481608  0.113161   -0.26823983 -1.21298744] 1.220703125e-05 419.5613456157606\n",
            "[-1.32888402  0.11274131 -0.26660137 -1.25652745] 1.220703125e-05 418.5528409540521\n",
            "[-1.32905429  0.11253041 -0.26659014 -1.25838526] 1.220703125e-05 418.5212531341664\n",
            "[-1.33037629  0.11257758 -0.26601743 -1.27290082] 1.220703125e-05 418.1845541023434\n",
            "[-1.33255181  0.11244621 -0.26514863 -1.29717195] 1.220703125e-05 417.64224252324556\n",
            "[-1.33473943  0.11253827 -0.26422637 -1.32207251] 6.103515625e-06 417.0864000867392\n",
            "[-1.33618083  0.11255691 -0.2636392  -1.33875926] 2.44140625e-05 416.7230198364778\n",
            "[-1.34006976  0.11218656 -0.26221122 -1.38493858] 1.220703125e-05 415.73112925206607\n",
            "[-1.3408256   0.11231964 -0.26188044 -1.39411568] 6.103515625e-06 415.5330711092626\n",
            "[-1.34127058  0.11210083 -0.26177213 -1.39955125] 1.220703125e-05 415.4250371508025\n",
            "[-1.34207878  0.11228622 -0.26140942 -1.4094824 ] 6.103515625e-06 415.2096087537129\n",
            "[-1.34306406  0.11212319 -0.26108167 -1.42169863] 1.220703125e-05 414.95672709311367\n",
            "[-1.34361846  0.11218566 -0.26085487 -1.42862446] 1.220703125e-05 414.8105699366283\n",
            "[-1.34447515  0.11216647 -0.26053914 -1.43940299] 1.220703125e-05 414.5876299304849\n",
            "[-1.34746357  0.1120122  -0.25948267 -1.47774082] 1.220703125e-05 413.8081773388609\n",
            "[-1.35071796  0.11221632 -0.25826056 -1.52085394] 1.220703125e-05 412.94913876171336\n",
            "[-1.35100104  0.112241   -0.25815411 -1.5246741 ] 1.220703125e-05 412.8755844801559\n",
            "[-1.3551901   0.11189347 -0.25681394 -1.58258695] 1.220703125e-05 411.75434910955374\n",
            "[-1.35556431  0.11210677 -0.25662748 -1.58788922] 2.44140625e-05 411.6506850037746\n",
            "[-1.35595203  0.11205054 -0.25651436 -1.593407  ] 6.103515625e-06 411.5452555459194\n",
            "[-1.35632492  0.11194883 -0.25641958 -1.59873637] 1.220703125e-05 411.44589944910854\n",
            "[-1.35815771  0.11205185 -0.25578877 -1.62525453] 6.103515625e-06 410.94866734500255\n",
            "[-1.35919192  0.11204911 -0.25545536 -1.64046298] 6.103515625e-06 410.6675446535382\n",
            "[-1.36023492  0.11205112 -0.25512215 -1.65598444] 6.103515625e-06 410.38318618376195\n",
            "[-1.36064387  0.11217922 -0.25495614 -1.66212094] 1.220703125e-05 410.2753027593785\n",
            "[-1.36193315  0.11187077 -0.25464118 -1.68166294] 1.220703125e-05 409.9241033354816\n",
            "[-1.36385656  0.11216818 -0.25396721 -1.711373  ] 1.220703125e-05 409.3910296568546\n",
            "[-1.36400747  0.11194995 -0.25398417 -1.71373393] 1.220703125e-05 409.34988894941887\n",
            "[-1.36435197  0.1119911  -0.25386895 -1.71913782] 1.220703125e-05 409.2533494434925\n",
            "[-1.36481867  0.111959   -0.25373841 -1.72649486] 1.220703125e-05 409.12573923327477\n",
            "[-1.36541384  0.11220514 -0.25349154 -1.73593682] 1.220703125e-05 408.96088435627183\n",
            "[-1.36664667  0.11227468 -0.25311085 -1.75571723] 1.220703125e-05 408.62099609877674\n",
            "[-1.36707181  0.11203683 -0.25305586 -1.76260989] 1.220703125e-05 408.4985990531463\n",
            "[-1.36739124  0.11204023 -0.25296286 -1.76781221] 1.220703125e-05 408.4096484462745\n",
            "[-1.36775902  0.11193209 -0.25288857 -1.7738281 ] 1.220703125e-05 408.31328203861653\n",
            "[-1.36830195  0.11232235 -0.2526236  -1.78275787] 1.220703125e-05 408.1600615321018\n",
            "[-1.36856949  0.1120818  -0.25261686 -1.78718259] 1.220703125e-05 408.0800420622514\n",
            "[-1.36858807  0.1122443  -0.25256529 -1.78748972] 1.220703125e-05 408.0757177742062\n",
            "[-1.36998271  0.11227071 -0.25217067 -1.81080156] 1.220703125e-05 407.6842497855549\n",
            "[-1.3712996   0.11208147 -0.25186802 -1.83320334] 1.220703125e-05 407.3157629662146\n",
            "[-1.37189302  0.11233421 -0.25163843 -1.84342371] 1.220703125e-05 407.14599002375485\n",
            "[-1.37218244  0.11212669 -0.25162118 -1.84843893] 1.220703125e-05 407.06544222470677\n",
            "[-1.3723079   0.11223634 -0.2515572  -1.85061794] 1.220703125e-05 407.02709895474817\n",
            "[-1.37269336  0.11233041 -0.2514297  -1.85733712] 2.44140625e-05 406.91858176387314\n",
            "[-1.37279202  0.11220526 -0.25143961 -1.85906302] 1.220703125e-05 406.8907214272498\n",
            "[-1.37383467  0.1122451  -0.25116103 -1.87743792] 1.220703125e-05 406.5946392385349\n",
            "1.220703125e-05 [-0.44802302 -8.14291106 -2.19878277 -8.01388628]\n",
            "[-1.3741111   0.11236813 -0.25105622 -1.88235267]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6KsBqtiMgiw"
      },
      "source": [
        "Learned weights from four runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5TzHN51Vwum"
      },
      "source": [
        "# [-1.63963568  0.11244618 -0.25229998 -1.83450268]\n",
        "# [-1.43047609  0.11237203 -0.25146534 -1.86348854]\n",
        "# [-0.90529042  0.11226296 -0.24705779 -2.05801113]\n",
        "# [-1.13011592  0.11231782 -0.24929065 -1.95950121]"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}